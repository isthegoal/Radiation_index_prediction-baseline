{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = pd.read_csv('data/train_feature.csv')\n",
    "train_label = pd.read_csv('data/train_label.csv')\n",
    "test_feature = pd.read_csv('data/test_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['日期', '时刻', '辐照度', '风速', '风向', '温度', '湿度', '气压'], dtype='object')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(data):\n",
    "    for i in range (8):   \n",
    "        train_feature_new = pd.DataFrame(columns= ['日期', '时刻', '辐照度', \n",
    "                                                   '风速', '风向', '温度', \n",
    "                                                   '湿度', '气压'])\n",
    "        for j in range(data.shape[0]//8):\n",
    "            train_feature_new = train_feature_new.append(data.iloc[i + j*8])\n",
    "            \n",
    "        train_feature_new = train_feature_new.drop(['时刻'],axis=1)\n",
    "        train_feature_new = train_feature_new.rename(index=str, columns={'辐照度':'辐照度_%d'%i, \n",
    "                                                     '风速':'风速_%d'%i, '风向':'风向_%d'%i, '温度':'温度_%d'%i, \n",
    "                                                    '湿度':'湿度_%d'%i, '气压':'气压_%d'%i})\n",
    "        if i == 0:\n",
    "            train_feature_all = train_feature_new\n",
    "        else:\n",
    "            train_feature_all = pd.merge(train_feature_all,train_feature_new,on = '日期')\n",
    "    return train_feature_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_all = data_process(test_feature)\n",
    "train_feature_all = data_process(train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_all['belong'] = 1\n",
    "train_feature_all['belong'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = pd.merge(train_feature_all,train_label,on='日期')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_all['电场实际太阳辐射指数'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data =train_all.append(test_feature_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv('data/all_data.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score,mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('data/all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    train_all = all_data.loc[all_data.belong == 0]\n",
    "    train_y = train_all['电场实际太阳辐射指数']\n",
    "    test_all = all_data.loc[all_data.belong == 1]\n",
    "    return train_all,train_y,test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm (all_data):\n",
    "    if os.path.exists('featurescore') == True:\n",
    "        shutil.rmtree('featurescore')\n",
    "    if os.path.exists('preds') == True:\n",
    "        shutil.rmtree('preds')\n",
    "\n",
    "\n",
    "    os.mkdir('featurescore')\n",
    "    os.mkdir('preds')\n",
    "    train_all,train_y,test_all = split_data(all_data)\n",
    "    date = test_all['日期'].values\n",
    "    \n",
    "    \n",
    "    mae_score = []\n",
    "    k = 5\n",
    "    skf = StratifiedKFold(n_splits=k,shuffle=True,random_state=1)\n",
    "    feats = [feature for feature in train_all.columns.values if feature not in ['日期','电场实际太阳辐射指数','belong']]\n",
    "    \n",
    "    train_x = train_all[feats]\n",
    "    test_x = test_all[feats]\n",
    "    \n",
    "    print('train_shape',train_x.shape)\n",
    "    print('test_shape',test_x.shape)\n",
    "    \n",
    "    for k,(train_k,valid_k) in enumerate(skf.split(train_x,np.zeros(shape=(train_x.shape[0], 1)))):\n",
    "        x_train,y_train,x_valid,y_valid = np.array(train_x)[train_k], np.array(train_y)[train_k], np.array(train_x)[valid_k], np.array(train_y)[valid_k]\n",
    "        print('###################### train!!! ################################')\n",
    "        gbm = lgb.LGBMRegressor(num_leaves=25,\n",
    "                                learning_rate=0.014,\n",
    "                                n_estimators=10000,\n",
    "                                max_depth=5,                #限制树模型的最大深度. 这可以在 #data 小的情况下防止过拟合.\n",
    "                               #min_data_in_leaf=24,         #一个叶子上数据的最小数量. 可以用来处理过拟合          \n",
    "                               #min_child_weight=1,         #一个叶子上的最小 hessian 和. 类似于 min_data_in_leaf, 可以用来处理过拟合\n",
    "                               #feature_fraction=0.8,       #如果 feature_fraction 小于 1.0, LightGBM 将会在每次迭代中随机选择部分特征. \n",
    "                               #           #例如, 如果设置为 0.8, 将会在每棵树训练之前选择 80% 的特征                             \n",
    "                               #bagging_fraction=0.8,       #类似于 feature_fraction, 但是它将在不进行重采样的情况下随机选择部分数据.\n",
    "                               #           #Note: 为了启用 bagging, bagging_freq 应该设置为非零值\n",
    "                               #bagging_freq=8             #bagging 的频率, 0 意味着禁用 bagging. k 意味着每 k 次迭代执行bagging)\n",
    "                               )\n",
    "\n",
    "\n",
    "        gbm.fit(x_train, y_train,\n",
    "                eval_set=[(x_valid, y_valid)],\n",
    "                eval_metric='mae',\n",
    "                early_stopping_rounds=100,\n",
    "                verbose = 100)\n",
    "\n",
    "        print('###################### valid!!! ##################################')\n",
    "        y_pred_val = gbm.predict(x_valid)\n",
    "        mae = mean_absolute_error(y_valid,y_pred_val)\n",
    "        print('The mae_score is',mae)\n",
    "        mae_score.append(mae)\n",
    "        print('###################### predict!!! ################################')\n",
    "        test_pred_y = gbm.predict(test_x)\n",
    "        test_result = pd.DataFrame(columns=[\"日期\",\"电场实际太阳辐射指数\"])\n",
    "        test_result['日期'] = date\n",
    "        test_result['电场实际太阳辐射指数'] = test_pred_y\n",
    "        test_result.to_csv(\"./preds/lgb{0}.csv\".format(k),index=None,encoding='utf-8')\n",
    "        \n",
    "\n",
    "            \n",
    "    print(np.mean(mae_score))\n",
    "    \n",
    "    #pred 取平均   \n",
    "    files = os.listdir('./preds')\n",
    "    pred = pd.read_csv('./preds/'+files[0])\n",
    "    pred_prob = pred['电场实际太阳辐射指数']\n",
    "    for f in files[1:]:\n",
    "        pred = pd.read_csv('./preds/'+f)\n",
    "        pred_prob += pred['电场实际太阳辐射指数']\n",
    "\n",
    "    pred_prob /= len(files)\n",
    "    #print(pred_prob)\n",
    "    pred_new = pd.DataFrame(date,columns=['time']).reset_index(drop = True)####注意索引问题\n",
    "    pred_new['prediction'] = pred_prob\n",
    "    pred_new.to_csv('preds/avg_preds.csv',index=False,encoding='utf-8')\n",
    "    return np.mean(mae_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_shape (2126, 48)\n",
      "test_shape (915, 48)\n",
      "###################### train!!! ################################\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l2: 0.0477051\tvalid_0's l1: 0.168063\n",
      "[200]\tvalid_0's l2: 0.0480502\tvalid_0's l1: 0.169028\n",
      "Early stopping, best iteration is:\n",
      "[138]\tvalid_0's l2: 0.0474327\tvalid_0's l1: 0.167456\n",
      "###################### valid!!! ##################################\n",
      "The mae_score is 0.16745585309975167\n",
      "###################### predict!!! ################################\n",
      "###################### train!!! ################################\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l2: 0.0517361\tvalid_0's l1: 0.179366\n",
      "[200]\tvalid_0's l2: 0.0509767\tvalid_0's l1: 0.17655\n",
      "[300]\tvalid_0's l2: 0.051111\tvalid_0's l1: 0.17645\n",
      "Early stopping, best iteration is:\n",
      "[200]\tvalid_0's l2: 0.0509767\tvalid_0's l1: 0.17655\n",
      "###################### valid!!! ##################################\n",
      "The mae_score is 0.17655046069617938\n",
      "###################### predict!!! ################################\n",
      "###################### train!!! ################################\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l2: 0.0499132\tvalid_0's l1: 0.176123\n",
      "[200]\tvalid_0's l2: 0.0488974\tvalid_0's l1: 0.174607\n",
      "Early stopping, best iteration is:\n",
      "[191]\tvalid_0's l2: 0.0488221\tvalid_0's l1: 0.174441\n",
      "###################### valid!!! ##################################\n",
      "The mae_score is 0.174440503454991\n",
      "###################### predict!!! ################################\n",
      "###################### train!!! ################################\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l2: 0.0479964\tvalid_0's l1: 0.167269\n",
      "[200]\tvalid_0's l2: 0.0489172\tvalid_0's l1: 0.168501\n",
      "Early stopping, best iteration is:\n",
      "[116]\tvalid_0's l2: 0.0478262\tvalid_0's l1: 0.167017\n",
      "###################### valid!!! ##################################\n",
      "The mae_score is 0.16701665409101785\n",
      "###################### predict!!! ################################\n",
      "###################### train!!! ################################\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l2: 0.0465577\tvalid_0's l1: 0.167462\n",
      "[200]\tvalid_0's l2: 0.0452334\tvalid_0's l1: 0.166207\n",
      "Early stopping, best iteration is:\n",
      "[168]\tvalid_0's l2: 0.0451143\tvalid_0's l1: 0.165635\n",
      "###################### valid!!! ##################################\n",
      "The mae_score is 0.16563500697645567\n",
      "###################### predict!!! ################################\n",
      "0.1702196956636791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1702196956636791"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightgbm(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "       importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "       n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "       random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "       subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'num_leaves': [27], 'learning_rate': [0.014], 'max_depth': [5], 'min_data_in_leaf': [24], 'min_child_weight': [3], 'feature_fraction': [0.8], 'bagging_fraction': [0.8], 'bagging_freq': [1, 2, 3, 4, 5, 6, 7, 8, 9]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "estimator = lgb.LGBMRegressor()\n",
    "param_grid = {\n",
    "    'num_leaves' : [27],\n",
    "    #'learning_rate' : [i/1000 for i in range(5,15)],\n",
    "    'learning_rate' : [0.014],\n",
    "    #'max_depth' : [i for i in range(3,7)],\n",
    "    'max_depth' : [5],\n",
    "    #'min_data_in_leaf' :[i for i in range(10,30)],\n",
    "    'min_data_in_leaf' :[24],\n",
    "    #'min_child_weight' : [i for i in range(1,4)],\n",
    "    'min_child_weight' : [3],#不好调，容易拟合\n",
    "    'feature_fraction' :[0.8],\n",
    "    'bagging_fraction':  [0.8],\n",
    "    'bagging_freq' : [8]\n",
    "}\n",
    "train_x,train_y,test_x = split_data(all_data)\n",
    "gbm = GridSearchCV(estimator, param_grid, cv=5,return_train_score=True)\n",
    "\n",
    "gbm.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.8,\n",
       " 'bagging_freq': 8,\n",
       " 'feature_fraction': 0.8,\n",
       " 'learning_rate': 0.014,\n",
       " 'max_depth': 5,\n",
       " 'min_child_weight': 3,\n",
       " 'min_data_in_leaf': 24,\n",
       " 'num_leaves': 27}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
